{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageCaptioningtoImageGeneration.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VJirYSWuLxk"
      },
      "source": [
        "# **Starter Notebook to Combine the Image Captioning Network Output to the Image Generator Network to Evaluate Performance of Image Caption**   \n",
        "\n",
        "Final Project - Mikayla Biggs, Kevin Steele, Austin Strom    \n",
        "AML 4/26/2021\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o_LxdNqueIX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxl2jY2bgEMF"
      },
      "source": [
        "### Approach: \n",
        "* pre-trained image captioning network model\n",
        "    * used as forward loss to improve T2F performance\n",
        "    * model essentially used as descriminator\n",
        "* V1 is only forward loss\n",
        "    * V2 forward loss + backward \n",
        "        * training T2F and image captioning at same time in case captioning bottlenecks performance in V1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUG98rcPZYmt"
      },
      "source": [
        "### Retreive Text2Face Repository and Data\n",
        "This is setup to get the v0.1 and v1.0 data from the google drive links. The v1.0 data has not been cleaned so the v0.1 should be used for initial testing of the base project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeLk57l9pVlM"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# You'll generate plots of attention in order to see which parts of an image\n",
        "# our model focuses on during captioning\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from PIL import Image"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz0P6w15ttrd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ccf08cf-2ba7-4145-a267-c8c721e433c1"
      },
      "source": [
        "!git clone https://github.com/austin-strom/T2F.git\n",
        "!gdown --id 1nD6kNAgIVjxpzIScJNLqUyRA1qEkc4Op\n",
        "!gdown --id 1cwcYbl0dhXEzmdbee_K_H6jcndbsxT2o\n",
        "\n",
        "!unzip -u -q face2text_v0.1.zip -d face2text_v0.1\n",
        "!unzip -u -q face2text_v1.0.zip -d face2text_v1.0\n",
        "\n",
        "# # This is moving the v0.1 file to the proper data dir for testing\n",
        "!mkdir T2F/data/LFW/Face2Text\n",
        "!mv face2text_v0.1/ T2F/data/LFW/Face2Text/.\n",
        "\n",
        "!wget http://vis-www.cs.umass.edu/lfw/lfw.tgz\n",
        "!tar -xf lfw.tgz\n",
        "!mv lfw T2F/data/LFW/."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'T2F' already exists and is not an empty directory.\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1nD6kNAgIVjxpzIScJNLqUyRA1qEkc4Op\n",
            "To: /content/face2text_v0.1.zip\n",
            "100% 156k/156k [00:00<00:00, 58.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1cwcYbl0dhXEzmdbee_K_H6jcndbsxT2o\n",
            "To: /content/face2text_v1.0.zip\n",
            "100% 217k/217k [00:00<00:00, 64.7MB/s]\n",
            "mkdir: cannot create directory ‘T2F/data/LFW/Face2Text’: File exists\n",
            "mv: cannot move 'face2text_v0.1/' to 'T2F/data/LFW/Face2Text/./face2text_v0.1': Directory not empty\n",
            "--2021-05-03 01:00:29--  http://vis-www.cs.umass.edu/lfw/lfw.tgz\n",
            "Resolving vis-www.cs.umass.edu (vis-www.cs.umass.edu)... 128.119.244.95\n",
            "Connecting to vis-www.cs.umass.edu (vis-www.cs.umass.edu)|128.119.244.95|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 180566744 (172M) [application/x-gzip]\n",
            "Saving to: ‘lfw.tgz.2’\n",
            "\n",
            "lfw.tgz.2           100%[===================>] 172.20M  81.7MB/s    in 2.1s    \n",
            "\n",
            "2021-05-03 01:00:31 (81.7 MB/s) - ‘lfw.tgz.2’ saved [180566744/180566744]\n",
            "\n",
            "mv: cannot move 'lfw' to 'T2F/data/LFW/./lfw': Directory not empty\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TixEeKykZaDj"
      },
      "source": [
        "# !pip uninstall pro-gan-pth==1.3.3"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTg8fsLfMZhI"
      },
      "source": [
        "# !git clone 'https://github.com/austin-strom/pro_gan_pytorch.git'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAU53DzypiyU",
        "outputId": "6855d2a9-9064-451b-ddda-b3b86d4eeb2b"
      },
      "source": [
        "# # This is moving the v0.1 file to the proper data dir for testing\n",
        "\n",
        "!mkdir face2text_v0.1/data\n",
        "# !mv face2text_v0.1/ Face2Text/.\n",
        "\n",
        "!wget http://vis-www.cs.umass.edu/lfw/lfw.tgz\n",
        "!tar -xf lfw.tgz\n",
        "!mv lfw face2text_v0.1/data/."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-03 01:00:34--  http://vis-www.cs.umass.edu/lfw/lfw.tgz\n",
            "Resolving vis-www.cs.umass.edu (vis-www.cs.umass.edu)... 128.119.244.95\n",
            "Connecting to vis-www.cs.umass.edu (vis-www.cs.umass.edu)|128.119.244.95|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 180566744 (172M) [application/x-gzip]\n",
            "Saving to: ‘lfw.tgz.3’\n",
            "\n",
            "lfw.tgz.3           100%[===================>] 172.20M  80.1MB/s    in 2.1s    \n",
            "\n",
            "2021-05-03 01:00:37 (80.1 MB/s) - ‘lfw.tgz.3’ saved [180566744/180566744]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdDz2MxxZd0E"
      },
      "source": [
        "%load_ext tensorboard\n",
        "import datetime, os"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AKGYTIVdfx_"
      },
      "source": [
        "!rm -rf T2F/implementation/runs"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RBD7t55uleO",
        "outputId": "374d3d61-eefb-4cb7-d001-9323e9899d5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone 'https://github.com/austin-strom/pro_gan_pytorch.git'\n",
        "!cd pro_gan_pytorch && git checkout revert_to_v1_3_3\n",
        "!mv pro_gan_pytorch/pro_gan_pytorch T2F/implementation/pro_gan_pytorch"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'pro_gan_pytorch' already exists and is not an empty directory.\n",
            "D\tpro_gan_pytorch/CustomLayers.py\n",
            "D\tpro_gan_pytorch/Losses.py\n",
            "D\tpro_gan_pytorch/PRO_GAN.py\n",
            "D\tpro_gan_pytorch/__init__.py\n",
            "Already on 'revert_to_v1_3_3'\n",
            "Your branch is up to date with 'origin/revert_to_v1_3_3'.\n",
            "mv: cannot stat 'pro_gan_pytorch/pro_gan_pytorch': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYfM1irIdiiT"
      },
      "source": [
        "# %cd T2F/implementation/\n",
        "\n",
        "# %tensorboard --logdir=runs\n",
        "\n",
        "# !mkdir training_runs\n",
        "# !mkdir training_runs/generated_samples training_runs/losses training_runs/saved_models\n",
        "# !python3 train_network.py --config=configs/1.conf\n",
        "\n",
        "# %cd ../../"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9IpTkslgQWG"
      },
      "source": [
        "import sys\n",
        "import datetime\n",
        "import time\n",
        "import torch as th\n",
        "import numpy as np\n",
        "import argparse\n",
        "import yaml\n",
        "import os\n",
        "import pickle\n",
        "import timeit\n",
        "import tensorflow as tf \n",
        "\n",
        "from torch.backends import cudnn\n",
        "\n",
        "# Append path to sys\n",
        "sys.path.append(\"T2F/implementation\")\n",
        "sys.path.append(\"T2F/implementation/networks\")\n",
        "sys.path.append(\"T2F/implementation/data_processing\")\n",
        "sys.path.append(\"T2F/implementation/pro_gan_pytorch\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4t4vQbokx1V",
        "outputId": "2dba28cb-c506-4ddb-ea35-a63911ea9f16"
      },
      "source": [
        "%cd T2F/implementation/\n",
        "\n",
        "import data_processing.DataLoader as dl\n",
        "\n",
        "# define the device for the training script\n",
        "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# set torch manual seed for consistent output\n",
        "th.manual_seed(3)\n",
        "\n",
        "# Start fast training mode:\n",
        "cudnn.benchmark = True\n",
        "logdir = \"runs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "file_writer = tf.summary.create_file_writer(logdir)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/T2F/implementation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGgAHdIJk52U"
      },
      "source": [
        "def  parse_keyword_arguments():\n",
        "  # pass dictionary into main\n",
        "  \"\"\"\n",
        "  command line arguments parser\n",
        "  :return: args => parsed command line arguments\n",
        "  \"\"\"\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument(\"--config\", action=\"store\", type=str, default=\"configs/1.conf\",\n",
        "    help=\"default configuration for the Network\")\n",
        "  parser.add_argument(\"--start_depth\", action=\"store\", type=int, default=0,\n",
        "    help=\"Starting depth for training the network\")\n",
        "  parser.add_argument(\"--encoder_file\", action=\"store\", type=str, default=None,\n",
        "    help=\"pretrained Encoder file (compatible with my code)\")\n",
        "  parser.add_argument(\"--ca_file\", action=\"store\", type=str, default=None,\n",
        "    help=\"pretrained Conditioning Augmentor file (compatible with my code)\")\n",
        "  parser.add_argument(\"--generator_file\", action=\"store\", type=str, default=None,\n",
        "    help=\"pretrained Generator file (compatible with my code)\")\n",
        "  parser.add_argument(\"--discriminator_file\", action=\"store\", type=str, default=None,\n",
        "    help=\"pretrained Discriminator file (compatible with my code)\")\n",
        "\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  return args"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR4dhTsVlEXE"
      },
      "source": [
        "def get_config(conf_file):\n",
        "  \"\"\"\n",
        "  parse and load the provided configuration\n",
        "  :param conf_file: configuration file\n",
        "  :return: conf => parsed configuration\n",
        "  \"\"\"\n",
        "  from easydict import EasyDict as edict\n",
        "\n",
        "  with open(conf_file, \"r\") as file_descriptor:\n",
        "      data = yaml.load(file_descriptor)\n",
        "\n",
        "  # convert the data into an easyDictionary\n",
        "  return edict(data)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E8oC7CnlIEZ"
      },
      "source": [
        "def create_grid(samples, scale_factor, img_file, real_imgs=False):\n",
        "  \"\"\"\n",
        "  utility function to create a grid of GAN samples\n",
        "  :param samples: generated samples for storing\n",
        "  :param scale_factor: factor for upscaling the image\n",
        "  :param img_file: name of file to write\n",
        "  :param real_imgs: turn off the scaling of images\n",
        "  :return: None (saves a file)\n",
        "  \"\"\"\n",
        "  from torchvision.utils import save_image\n",
        "  from torch.nn.functional import interpolate\n",
        "\n",
        "  samples = th.clamp((samples / 2) + 0.5, min=0, max=1)\n",
        "  # print(samples)\n",
        "\n",
        "  # upsample the image\n",
        "  if not real_imgs and scale_factor > 1:\n",
        "      samples = interpolate(samples,\n",
        "                            scale_factor=scale_factor)\n",
        "      \n",
        "  # call new captioning method on samples for new loss calculation\n",
        "  generated_captions = evaluate(samples.detach())\n",
        "\n",
        "  # print(\"Image Caption Loss: \", generated_captions)\n",
        "\n",
        "\n",
        "  # save the images:\n",
        "  save_image(samples, img_file, nrow=int(np.sqrt(len(samples))))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKjD-zsvlOsd"
      },
      "source": [
        "def create_descriptions_file(file, captions, dataset):\n",
        "  \"\"\"\n",
        "  utility function to create a file for storing the captions\n",
        "  :param file: file for storing the captions\n",
        "  :param captions: encoded_captions or raw captions\n",
        "  :param dataset: the dataset object for transforming captions\n",
        "  :return: None (saves a file)\n",
        "  \"\"\"\n",
        "  from functools import reduce\n",
        "\n",
        "  # transform the captions to text:\n",
        "  if isinstance(captions, th.Tensor):\n",
        "      captions = list(map(lambda x: dataset.get_english_caption(x.cpu()),\n",
        "                          [captions[i] for i in range(captions.shape[0])]))\n",
        "\n",
        "      with open(file, \"w\") as filler:\n",
        "          for caption in captions:\n",
        "              filler.write(reduce(lambda x, y: x + \" \" + y, caption, \"\"))\n",
        "              filler.write(\"\\n\\n\")\n",
        "  else:\n",
        "      with open(file, \"w\") as filler:\n",
        "          for caption in captions:\n",
        "              filler.write(caption)\n",
        "              filler.write(\"\\n\\n\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJtIJ1aJFwfr"
      },
      "source": [
        "### Add step in loss function to compare intermediate image output result from image captioning network   \n",
        "\n",
        "Acts as adition metric to tune loss and ideally improve generated image quality.   \n",
        "\n",
        "**Where?**    \n",
        "In pr_gan_pytorch project by Akanimax, output from training image generator - currently using wpgan loss. \n",
        "\n",
        "**TODO:**   \n",
        "Can't find where any validation/testing is being done to take intermediate generated images - need to investigate training loop with Austin to see if this can be added to change the loss function and add the image captioning network to the pipeline\n",
        "\n",
        "[ProGAN Loss Functons - Akanimax T2F](https://github.com/akanimax/pro_gan_pytorch/blob/cdd9002ad171ee47c65c3670318473a76eb682e2/pro_gan_pytorch/losses.py#L35)   \n",
        "\n",
        "[Word Importance - Akanimax T2F](https://github.com/austin-strom/T2F/blob/master/implementation/networks/InferSent/encoder/demo.ipynb)   \n",
        "\n",
        "\n",
        "[Evaluate Method - Image Captioning](https://github.com/austin-strom/text2face/blob/main/TFImageCap.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC-A0eZvuZJN"
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, x):\n",
        "    features = x[0]\n",
        "    hidden = x[1]\n",
        "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
        "\n",
        "    # hidden shape == (batch_size, hidden_size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "    # attention_hidden_layer shape == (batch_size, 64, units)\n",
        "    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
        "                                         self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # score shape == (batch_size, 64, 1)\n",
        "    # This gives you an unnormalized score for each image feature.\n",
        "    score = self.V(attention_hidden_layer)\n",
        "\n",
        "    # attention_weights shape == (batch_size, 64, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * features\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96PjfIEqueI7"
      },
      "source": [
        "class RNN_Decoder(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units, vocab_size):\n",
        "    super(RNN_Decoder, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "  def call(self, x):\n",
        "    features = x[1]\n",
        "    hidden = x[2]\n",
        "    x = x[0]\n",
        "    # defining attention as a separate model\n",
        "    context_vector, attention_weights = self.attention([features, hidden])\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # shape == (batch_size, max_length, hidden_size)\n",
        "    x = self.fc1(output)\n",
        "\n",
        "    # x shape == (batch_size * max_length, hidden_size)\n",
        "    x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size * max_length, vocab)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "  def reset_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYgV4EIpuitr"
      },
      "source": [
        "class CNN_Encoder(tf.keras.Model):\n",
        "    # Since you have already extracted the features and dumped it\n",
        "    # This encoder passes those features through a Fully connected layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        # shape after fc == (batch_size, 64, embedding_dim)\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCKal8KViKcN"
      },
      "source": [
        "# Feel free to change these parameters according to your system's configuration\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "vocab_size = 5000 + 1\n",
        "# num_steps = len(img_name_train) // BATCH_SIZE\n",
        "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
        "# These two variables represent that vector shape\n",
        "features_shape = 2048\n",
        "attention_features_shape = 64"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDvh8q0-wli8"
      },
      "source": [
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
        "\n",
        "encoder.built=True\n",
        "decoder.built=True"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2AL6Vq3vDPD",
        "outputId": "c5961c8d-ebce-4216-e151-5c88f3152bb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd ../../\n",
        "!pwd\n",
        "!git clone https://github.com/austin-strom/text2face.git\n",
        "!mkdir encoder\n",
        "!mkdir decoder\n",
        "!mv text2face/ImgCapModel/decoder* decoder/.\n",
        "!mv text2face/ImgCapModel/encoder* encoder/.\n",
        "!mv text2face/ImgCapModel/tokenizer.pickle .\n",
        "\n",
        "%cd T2F/implementation/"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content\n",
            "Cloning into 'text2face'...\n",
            "remote: Enumerating objects: 69, done.\u001b[K\n",
            "remote: Counting objects: 100% (69/69), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 69 (delta 31), reused 21 (delta 9), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (69/69), done.\n",
            "/content/T2F/implementation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbG3EVq2jjIk"
      },
      "source": [
        "saving_encoder_path = '/content/encoder/encoder'\n",
        "saving_decoder_path = '/content/decoder/decoder'\n",
        "saving_tokenizer_path = '/content/tokenizer.pickle'\n",
        "load_model = True"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSIiO4nWi5RO"
      },
      "source": [
        "import pickle as pkl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "# from TFImageCap import RNN_Decoder, CNN_Encoder\n",
        "if load_model:\n",
        "  encoder.load_weights(saving_encoder_path)\n",
        "  decoder.load_weights(saving_decoder_path)\n",
        "\n",
        "  # loading\n",
        "  with open(saving_tokenizer_path, 'rb') as handle:\n",
        "      tokenizer = pickle.load(handle)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaNkXreLjRKQ",
        "outputId": "4a31f969-1795-4913-8a22-9ecdfb5af28c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
        "                                                weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOmCgENRFo6P"
      },
      "source": [
        "# compare words generated from image captioning network and original caption to \n",
        "# see if the image generator is preserving key features of the original subject\n",
        "def eval_key_words(im_cap, true_cap):\n",
        "  # 1. identify important words (maybe use model to eval word importance like RF)\n",
        "  # 2. calculate some difference metric like MSE\n",
        "  # 3. return the metric to be used in GAN loss fxn\n",
        "\n",
        "  return\n",
        "\n",
        "# Should be the same as what is used in the image captioning network Kevin worked on\n",
        "# input image is intermediate output from text-to-image GAN generator\n",
        "def load_image(img):\n",
        "  img = np.asarray(img.cpu()).transpose(1,2,0)\n",
        "  img = tf.image.resize(img, (128, 128))\n",
        "  img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "  return img\n",
        "\n",
        "# get the image caption for the intermediate generated output image\n",
        "# NOTE: may need more inputs based on what is needed for the captioning network\n",
        "# NOTE: need to use the evaluate method from TFImageCap notebook to generate caption\n",
        "def evaluate(images):\n",
        "  result = list()\n",
        "\n",
        "  for im in images:\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    temp_input = tf.expand_dims(load_image(im), 0)\n",
        "    print(\"temp_input: \", np.shape(temp_input))\n",
        "    img_tensor_val = image_features_extract_model(temp_input)\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n",
        "                                              -1, img_tensor_val.shape[3]))\n",
        "    features = encoder(img_tensor_val)\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "    current_im_result = []\n",
        "\n",
        "    for i in range(94):\n",
        "      predictions, hidden, attention_weights = decoder([dec_input,\n",
        "                                                        features,\n",
        "                                                        hidden])\n",
        "\n",
        "      predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "\n",
        "      if predicted_id not in tokenizer.index_word.keys():\n",
        "        predicted_id = 0\n",
        "\n",
        "      current_im_result.append(tokenizer.index_word[predicted_id])\n",
        "\n",
        "      if tokenizer.index_word[predicted_id] == '<end>':\n",
        "        result.append(current_im_result)\n",
        "        break\n",
        "\n",
        "      dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    result.append(current_im_result)\n",
        "\n",
        "  return result"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8znkFwAlRiJ"
      },
      "source": [
        "def train_networks(encoder, ca, c_pro_gan, dataset, epochs,\n",
        "                  encoder_optim, ca_optim, fade_in_percentage,\n",
        "                  batch_sizes, start_depth, num_workers, feedback_factor,\n",
        "                  log_dir, sample_dir, checkpoint_factor,\n",
        "                  save_dir, use_matching_aware_dis=True):\n",
        "  # required only for type checking\n",
        "  from networks.TextEncoder import PretrainedEncoder\n",
        "\n",
        "  # input assertions\n",
        "  assert c_pro_gan.depth == len(batch_sizes), \"batch_sizes not compatible with depth\"\n",
        "  assert c_pro_gan.depth == len(epochs), \"epochs_sizes not compatible with depth\"\n",
        "  assert c_pro_gan.depth == len(fade_in_percentage), \"fip_sizes not compatible with depth\"\n",
        "\n",
        "  # put all the Networks in training mode:\n",
        "  ca.train()\n",
        "  c_pro_gan.gen.train()\n",
        "  c_pro_gan.dis.train()\n",
        "\n",
        "  if not isinstance(encoder, PretrainedEncoder):\n",
        "      encoder.train()\n",
        "\n",
        "  print(\"Starting the training process ... \")\n",
        "\n",
        "  # create fixed_input for debugging\n",
        "  temp_data = dl.get_data_loader(dataset, batch_sizes[start_depth], num_workers=3)\n",
        "  fixed_captions, fixed_real_images = iter(temp_data).next()\n",
        "\n",
        "  fixed_real_images = fixed_real_images.to(device)\n",
        "  fixed_captions = fixed_captions.to(device)\n",
        "\n",
        "  fixed_embeddings = encoder(fixed_captions)\n",
        "\n",
        "  fixed_embeddings = (fixed_embeddings).to(device)\n",
        "\n",
        "  fixed_c_not_hats, _, _ = ca(fixed_embeddings)\n",
        "\n",
        "  fixed_noise = th.randn(len(fixed_captions),\n",
        "                          c_pro_gan.latent_size - fixed_c_not_hats.shape[-1]).to(device)\n",
        "\n",
        "  fixed_gan_input = th.cat((fixed_c_not_hats, fixed_noise), dim=-1)\n",
        "\n",
        "  # save the fixed_images once:\n",
        "  fixed_save_dir = os.path.join(sample_dir, \"__Real_Info\")\n",
        "  os.makedirs(fixed_save_dir, exist_ok=True)\n",
        "  create_grid(fixed_real_images, None,  # scale factor is not required here\n",
        "              os.path.join(fixed_save_dir, \"real_samples.png\"), real_imgs=True)\n",
        "  create_descriptions_file(os.path.join(fixed_save_dir, \"real_captions.txt\"),\n",
        "                            fixed_captions,\n",
        "                            dataset)\n",
        "\n",
        "  # create a global time counter\n",
        "  global_time = time.time()\n",
        "\n",
        "  # delete temp data loader:\n",
        "  del temp_data\n",
        "\n",
        "  for current_depth in range(start_depth, c_pro_gan.depth):\n",
        "\n",
        "      print(\"\\n\\nCurrently working on Depth: \", current_depth)\n",
        "      current_res = np.power(2, current_depth + 2)\n",
        "      print(\"Current resolution: %d x %d\" % (current_res, current_res))\n",
        "\n",
        "      data = dl.get_data_loader(dataset, batch_sizes[current_depth], num_workers)\n",
        "\n",
        "      ticker = 1\n",
        "\n",
        "      for epoch in range(1, epochs[current_depth] + 1):\n",
        "          start = timeit.default_timer()  # record time at the start of epoch\n",
        "\n",
        "          print(\"\\nEpoch: %d\" % epoch)\n",
        "          total_batches = len(iter(data))\n",
        "          fader_point = int((fade_in_percentage[current_depth] / 100)\n",
        "                            * epochs[current_depth] * total_batches)\n",
        "\n",
        "          for (i, batch) in enumerate(data, 1):\n",
        "              # calculate the alpha for fading in the layers\n",
        "              alpha = ticker / fader_point if ticker <= fader_point else 1\n",
        "\n",
        "              # extract current batch of data for training\n",
        "              captions, images = batch\n",
        "\n",
        "              if encoder_optim is not None:\n",
        "                  captions = captions.to(device)\n",
        "\n",
        "              images = images.to(device)\n",
        "\n",
        "              # perform text_work:\n",
        "              embeddings = encoder(captions).to(device)\n",
        "              if encoder_optim is None:\n",
        "                  # detach the LSTM from backpropagation\n",
        "                  embeddings = embeddings.detach()\n",
        "              c_not_hats, mus, sigmas = ca(embeddings)\n",
        "\n",
        "              z = th.randn(\n",
        "                  len(captions),\n",
        "                  c_pro_gan.latent_size - c_not_hats.shape[-1]\n",
        "              ).to(device)\n",
        "\n",
        "              gan_input = th.cat((c_not_hats, z), dim=-1)\n",
        "\n",
        "              # optimize the discriminator:\n",
        "              dis_loss = c_pro_gan.optimize_discriminator(gan_input, images,\n",
        "                                                          embeddings.detach(),\n",
        "                                                          current_depth, alpha,\n",
        "                                                          use_matching_aware_dis)\n",
        "\n",
        "              # optimize the generator:\n",
        "              z = th.randn(\n",
        "                  captions.shape[0] if isinstance(captions, th.Tensor) else len(captions),\n",
        "                  c_pro_gan.latent_size - c_not_hats.shape[-1]\n",
        "              ).to(device)\n",
        "\n",
        "              gan_input = th.cat((c_not_hats, z), dim=-1)\n",
        "\n",
        "              if encoder_optim is not None:\n",
        "                  encoder_optim.zero_grad()\n",
        "\n",
        "              ca_optim.zero_grad()\n",
        "              gen_loss = c_pro_gan.optimize_generator(gan_input, embeddings,\n",
        "                                                      current_depth, alpha)\n",
        "\n",
        "              # once the optimize_generator is called, it also sends gradients\n",
        "              # to the Conditioning Augmenter and the TextEncoder. Hence the\n",
        "              # zero_grad statements prior to the optimize_generator call\n",
        "              # now perform optimization on those two as well\n",
        "              # obtain the loss (KL divergence from ca_optim)\n",
        "              kl_loss = th.mean(0.5 * th.sum((mus ** 2) + (sigmas ** 2)\n",
        "                                              - th.log((sigmas ** 2)) - 1, dim=1))\n",
        "              kl_loss.backward()\n",
        "              ca_optim.step()\n",
        "              if encoder_optim is not None:\n",
        "                  encoder_optim.step()\n",
        "\n",
        "              # provide a loss feedback\n",
        "              if i % int(total_batches / feedback_factor) == 0 or i == 1:\n",
        "                  elapsed = time.time() - global_time\n",
        "                  elapsed = str(datetime.timedelta(seconds=elapsed))\n",
        "                  print(\"Elapsed [%s]  batch: %d  d_loss: %f  g_loss: %f  kl_los: %f\"\n",
        "                        % (elapsed, i, dis_loss, gen_loss, kl_loss.item()))\n",
        "\n",
        "                  # also write the losses to the log file:\n",
        "                  os.makedirs(log_dir, exist_ok=True)\n",
        "                  log_file = os.path.join(log_dir, \"loss_\" + str(current_depth) + \".log\")\n",
        "                  with open(log_file, \"a\") as log:\n",
        "                      log.write(str(dis_loss) + \"\\t\" + str(gen_loss)\n",
        "                                + \"\\t\" + str(kl_loss.item()) + \"\\n\")\n",
        "\n",
        "                  # create a grid of samples and save it\n",
        "                  gen_img_file = os.path.join(sample_dir, \"gen_\" + str(current_depth) +\n",
        "                                              \"_\" + str(epoch) + \"_\" +\n",
        "                                              str(i) + \".png\")\n",
        "\n",
        "                  create_grid(\n",
        "                      samples=c_pro_gan.gen(\n",
        "                          fixed_gan_input,\n",
        "                          current_depth,\n",
        "                          alpha\n",
        "                      ),\n",
        "                      scale_factor=int(np.power(2, c_pro_gan.depth - current_depth - 1)),\n",
        "                      img_file=gen_img_file,\n",
        "                  )\n",
        "\n",
        "              # increment the ticker:\n",
        "              ticker += 1\n",
        "\n",
        "          stop = timeit.default_timer()\n",
        "          print(\"Time taken for epoch: %.3f secs\" % (stop - start))\n",
        "\n",
        "          if epoch % checkpoint_factor == 0 or epoch == 0:\n",
        "              # save the Model\n",
        "              encoder_save_file = os.path.join(save_dir, \"Encoder_\" +\n",
        "                                                str(current_depth) + \".pth\")\n",
        "              ca_save_file = os.path.join(save_dir, \"Condition_Augmentor_\" +\n",
        "                                          str(current_depth) + \".pth\")\n",
        "              gen_save_file = os.path.join(save_dir, \"GAN_GEN_\" +\n",
        "                                            str(current_depth) + \".pth\")\n",
        "              dis_save_file = os.path.join(save_dir, \"GAN_DIS_\" +\n",
        "                                            str(current_depth) + \".pth\")\n",
        "\n",
        "              os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "              if encoder_optim is not None:\n",
        "                  th.save(encoder.state_dict(), encoder_save_file, pickle)\n",
        "              th.save(ca.state_dict(), ca_save_file, pickle)\n",
        "              th.save(c_pro_gan.gen.state_dict(), gen_save_file, pickle)\n",
        "              th.save(c_pro_gan.dis.state_dict(), dis_save_file, pickle)\n",
        "\n",
        "  print(\"Training completed ...\")\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jule2aT1gygA"
      },
      "source": [
        "def main(args):\n",
        "  \"\"\"\n",
        "  Main function for the script\n",
        "  :param args: all args from cmdl as a dictionary (key,val) \n",
        "  :return: None\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"Using Device:\", device)\n",
        "  from networks.TextEncoder import Encoder\n",
        "  from networks.ConditionAugmentation import ConditionAugmentor\n",
        "  from pro_gan_pytorch.PRO_GAN import ConditionalProGAN\n",
        "\n",
        "  print(args['config'])\n",
        "  config = get_config(args['config'])\n",
        "  print(\"Current Configuration:\", config)\n",
        "\n",
        "  # create the dataset for training\n",
        "  if config.use_pretrained_encoder:\n",
        "      dataset = dl.RawTextFace2TextDataset(\n",
        "          annots_file=config.annotations_file,\n",
        "          img_dir=config.images_dir,\n",
        "          img_transform=dl.get_transform(config.img_dims)\n",
        "      )\n",
        "      from networks.TextEncoder import PretrainedEncoder\n",
        "      # create a new session object for the pretrained encoder:\n",
        "      text_encoder = PretrainedEncoder(\n",
        "          model_file=config.pretrained_encoder_file,\n",
        "          embedding_file=config.pretrained_embedding_file,\n",
        "          device=device\n",
        "      )\n",
        "      encoder_optim = None\n",
        "  else:\n",
        "      dataset = dl.Face2TextDataset(\n",
        "          pro_pick_file=config.processed_text_file,\n",
        "          img_dir=config.images_dir,\n",
        "          img_transform=dl.get_transform(config.img_dims),\n",
        "          captions_len=config.captions_length\n",
        "      )\n",
        "      text_encoder = Encoder(\n",
        "          embedding_size=config.embedding_size,\n",
        "          vocab_size=dataset.vocab_size,\n",
        "          hidden_size=config.hidden_size,\n",
        "          num_layers=config.num_layers,\n",
        "          device=device\n",
        "      )\n",
        "      encoder_optim = th.optim.Adam(text_encoder.parameters(),\n",
        "                                    lr=config.learning_rate,\n",
        "                                    betas=(config.beta_1, config.beta_2),\n",
        "                                    eps=config.eps)\n",
        "\n",
        "  # create the networks\n",
        "\n",
        "  if args['encoder_file'] is not None:\n",
        "      # Note this should not be used with the pretrained encoder file\n",
        "      print(\"Loading encoder from:\", args['encoder_file'])\n",
        "      text_encoder.load_state_dict(th.load(args['encoder_file']))\n",
        "\n",
        "  condition_augmenter = ConditionAugmentor(\n",
        "      input_size=config.hidden_size,\n",
        "      latent_size=config.ca_out_size,\n",
        "      use_eql=config.use_eql,\n",
        "      device=device\n",
        "  )\n",
        "\n",
        "  if args['ca_file'] is not None:\n",
        "      print(\"Loading conditioning augmenter from:\", args['ca_file'])\n",
        "      condition_augmenter.load_state_dict(th.load(args['ca_file']))\n",
        "\n",
        "  c_pro_gan = ConditionalProGAN(\n",
        "      embedding_size=config.hidden_size,\n",
        "      depth=config.depth,\n",
        "      latent_size=config.latent_size,\n",
        "      compressed_latent_size=config.compressed_latent_size,\n",
        "      learning_rate=config.learning_rate,\n",
        "      beta_1=config.beta_1,\n",
        "      beta_2=config.beta_2,\n",
        "      eps=config.eps,\n",
        "      drift=config.drift,\n",
        "      n_critic=config.n_critic,\n",
        "      use_eql=config.use_eql,\n",
        "      loss=config.loss_function,\n",
        "      use_ema=config.use_ema,\n",
        "      ema_decay=config.ema_decay,\n",
        "      device=device\n",
        "  )\n",
        "\n",
        "  if args['generator_file'] is not None:\n",
        "      print(\"Loading generator from:\", args['generator_file'])\n",
        "      c_pro_gan.gen.load_state_dict(th.load(args['generator_file']))\n",
        "\n",
        "  if args['discriminator_file'] is not None:\n",
        "      print(\"Loading discriminator from:\", args['discriminator_file'])\n",
        "      c_pro_gan.dis.load_state_dict(th.load(args['discriminator_file']))\n",
        "\n",
        "  # create the optimizer for Condition Augmenter separately\n",
        "  ca_optim = th.optim.Adam(condition_augmenter.parameters(),\n",
        "                            lr=config.learning_rate,\n",
        "                            betas=(config.beta_1, config.beta_2),\n",
        "                            eps=config.eps)\n",
        "\n",
        "  print(\"Generator Config:\")\n",
        "  print(c_pro_gan.gen)\n",
        "\n",
        "  print(\"\\nDiscriminator Config:\")\n",
        "  print(c_pro_gan.dis)\n",
        "\n",
        "  # train all the networks\n",
        "  train_networks(\n",
        "      encoder=text_encoder,\n",
        "      ca=condition_augmenter,\n",
        "      c_pro_gan=c_pro_gan,\n",
        "      dataset=dataset,\n",
        "      encoder_optim=encoder_optim,\n",
        "      ca_optim=ca_optim,\n",
        "      epochs=config.epochs,\n",
        "      fade_in_percentage=config.fade_in_percentage,\n",
        "      start_depth=args['start_depth'],\n",
        "      batch_sizes=config.batch_sizes,\n",
        "      num_workers=config.num_workers,\n",
        "      feedback_factor=config.feedback_factor,\n",
        "      log_dir=config.log_dir,\n",
        "      sample_dir=config.sample_dir,\n",
        "      checkpoint_factor=config.checkpoint_factor,\n",
        "      save_dir=config.save_dir,\n",
        "      use_matching_aware_dis=config.use_matching_aware_discriminator\n",
        "  )"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyJVM9d2vU_K",
        "outputId": "ad8068d7-ddd7-41d8-e047-b51612e7646b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/T2F/implementation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RHTyLwnYji4_",
        "outputId": "81d5882b-848f-43d4-bdbb-8f4855a21bcc"
      },
      "source": [
        "args = {\n",
        "    'config': 'configs/1.conf',\n",
        "    'start_depth': 0,\n",
        "    'encoder_file': None,\n",
        "    'ca_file': None,\n",
        "    'generator_file': None,\n",
        "    'discriminator_file': None\n",
        "}\n",
        "\n",
        "main(args)\n",
        "\n",
        "%cd ../../"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Device: cuda\n",
            "configs/1.conf\n",
            "Current Configuration: {'images_dir': '../data/LFW/lfw', 'processed_text_file': 'processed_annotations/processed_text.pkl', 'annotations_file': '../data/LFW/Face2Text/face2text_v0.1/clean.json', 'pretrained_encoder_dir': 'tf-hub_modules/text_encoder/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47', 'log_dir': 'training_runs/1/losses/', 'sample_dir': 'training_runs/1/generated_samples/', 'save_dir': 'training_runs/1/saved_models/', 'captions_length': 100, 'img_dims': [256, 256], 'download_pretrained_encoder': False, 'use_pretrained_encoder': False, 'p_proc_gpu_mem': 0.3, 'embedding_size': 128, 'hidden_size': 512, 'num_layers': 3, 'ca_out_size': 256, 'compressed_latent_size': 128, 'use_eql': True, 'use_ema': True, 'ema_decay': 0.999, 'depth': 7, 'latent_size': 512, 'learning_rate': 0.001, 'beta_1': 0, 'beta_2': 0.99, 'eps': 1e-08, 'drift': 0.001, 'n_critic': 1, 'epochs': [20, 40, 40, 40, 40, 40, 40], 'fade_in_percentage': [50, 50, 50, 50, 50, 50, 50], 'batch_sizes': [16, 16, 16, 16, 16, 16, 14], 'loss_function': 'wgan-gp', 'num_workers': 3, 'feedback_factor': 7, 'checkpoint_factor': 1, 'use_matching_aware_discriminator': True}\n",
            "Generator Config:\n",
            "Generator(\n",
            "  (initial_block): GenInitialBlock(\n",
            "    (conv_1): _equalized_deconv2d(\n",
            "      (deconv): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    )\n",
            "    (conv_2): _equalized_conv2d(\n",
            "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    )\n",
            "    (lrelu): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (layers): ModuleList(\n",
            "    (0): GenGeneralConvBlock(\n",
            "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
            "      (conv_1): _equalized_conv2d(\n",
            "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv_2): _equalized_conv2d(\n",
            "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (lrelu): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (1): GenGeneralConvBlock(\n",
            "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
            "      (conv_1): _equalized_conv2d(\n",
            "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv_2): _equalized_conv2d(\n",
            "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (lrelu): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (2): GenGeneralConvBlock(\n",
            "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
            "      (conv_1): _equalized_conv2d(\n",
            "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv_2): _equalized_conv2d(\n",
            "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (lrelu): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (3): GenGeneralConvBlock(\n",
            "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
            "      (conv_1): _equalized_conv2d(\n",
            "        (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv_2): _equalized_conv2d(\n",
            "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (lrelu): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (4): GenGeneralConvBlock(\n",
            "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
            "      (conv_1): _equalized_conv2d(\n",
            "        (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv_2): _equalized_conv2d(\n",
            "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (lrelu): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (5): GenGeneralConvBlock(\n",
            "      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
            "      (conv_1): _equalized_conv2d(\n",
            "        (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv_2): _equalized_conv2d(\n",
            "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (lrelu): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "  )\n",
            "  (rgb_converters): ModuleList(\n",
            "    (0): _equalized_conv2d(\n",
            "      (conv): Conv2d(512, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (1): _equalized_conv2d(\n",
            "      (conv): Conv2d(512, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (2): _equalized_conv2d(\n",
            "      (conv): Conv2d(512, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (3): _equalized_conv2d(\n",
            "      (conv): Conv2d(512, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (4): _equalized_conv2d(\n",
            "      (conv): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (5): _equalized_conv2d(\n",
            "      (conv): Conv2d(128, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (6): _equalized_conv2d(\n",
            "      (conv): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (temporaryUpsampler): Upsample(scale_factor=2.0, mode=nearest)\n",
            ")\n",
            "\n",
            "Discriminator Config:\n",
            "ConditionalDiscriminator(\n",
            "  (final_block): ConDisFinalBlock(\n",
            "    (batch_discriminator): MinibatchStdDev()\n",
            "    (compressor): _equalized_linear(\n",
            "      (linear): Linear(in_features=512, out_features=128, bias=True)\n",
            "    )\n",
            "    (conv_1): _equalized_conv2d(\n",
            "      (conv): Conv2d(513, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    )\n",
            "    (conv_2): _equalized_conv2d(\n",
            "      (conv): Conv2d(640, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (conv_3): _equalized_conv2d(\n",
            "      (conv): Conv2d(512, 512, kernel_size=(4, 4), stride=(1, 1))\n",
            "    )\n",
            "    (conv_4): _equalized_conv2d(\n",
            "      (conv): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (lrelu): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (layers): ModuleList(\n",
            "    (0): DisGeneralConvBlock(\n",
            "      (conv_1): _equalized_conv2d(\n",
            "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv_2): _equalized_conv2d(\n",
            "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (downSampler): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "      (lrelu): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (1): DisGeneralConvBlock(\n",
            "      (conv_1): _equalized_conv2d(\n",
            "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv_2): _equalized_conv2d(\n",
            "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (downSampler): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "      (lrelu): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (2): DisGeneralConvBlock(\n",
            "      (conv_1): _equalized_conv2d(\n",
            "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv_2): _equalized_conv2d(\n",
            "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (downSampler): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "      (lrelu): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (3): DisGeneralConvBlock(\n",
            "      (conv_1): _equalized_conv2d(\n",
            "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv_2): _equalized_conv2d(\n",
            "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (downSampler): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "      (lrelu): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (4): DisGeneralConvBlock(\n",
            "      (conv_1): _equalized_conv2d(\n",
            "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv_2): _equalized_conv2d(\n",
            "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (downSampler): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "      (lrelu): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (5): DisGeneralConvBlock(\n",
            "      (conv_1): _equalized_conv2d(\n",
            "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (conv_2): _equalized_conv2d(\n",
            "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (downSampler): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "      (lrelu): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "  )\n",
            "  (rgb_to_features): ModuleList(\n",
            "    (0): _equalized_conv2d(\n",
            "      (conv): Conv2d(3, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (1): _equalized_conv2d(\n",
            "      (conv): Conv2d(3, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (2): _equalized_conv2d(\n",
            "      (conv): Conv2d(3, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (3): _equalized_conv2d(\n",
            "      (conv): Conv2d(3, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (4): _equalized_conv2d(\n",
            "      (conv): Conv2d(3, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (5): _equalized_conv2d(\n",
            "      (conv): Conv2d(3, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (6): _equalized_conv2d(\n",
            "      (conv): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (temporaryDownsampler): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            ")\n",
            "Starting the training process ... \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "temp_input:  (1, 128, 128, 3)\n",
            "temp_input:  (1, 128, 128, 3)\n",
            "temp_input:  (1, 128, 128, 3)\n",
            "temp_input:  (1, 128, 128, 3)\n",
            "temp_input:  (1, 128, 128, 3)\n",
            "temp_input:  (1, 128, 128, 3)\n",
            "temp_input:  (1, 128, 128, 3)\n",
            "temp_input:  (1, 128, 128, 3)\n",
            "temp_input:  (1, 128, 128, 3)\n",
            "temp_input:  (1, 128, 128, 3)\n",
            "temp_input:  (1, 128, 128, 3)\n",
            "temp_input:  (1, 128, 128, 3)\n",
            "temp_input:  (1, 128, 128, 3)\n",
            "temp_input:  (1, 128, 128, 3)\n",
            "temp_input:  (1, 128, 128, 3)\n",
            "temp_input:  (1, 128, 128, 3)\n",
            "\n",
            "\n",
            "Currently working on Depth:  0\n",
            "Current resolution: 4 x 4\n",
            "\n",
            "Epoch: 1\n",
            "Elapsed [0:00:01.237357]  batch: 1  d_loss: 8.546745  g_loss: 0.129311  kl_los: 3982.495361\n",
            "temp_input:  (1, 128, 128, 3)\n",
            "temp_input:  (1, 128, 128, 3)\n",
            "temp_input:  (1, 128, 128, 3)\n",
            "temp_input:  (1, 128, 128, 3)\n",
            "temp_input:  (1, 128, 128, 3)\n",
            "temp_input:  (1, 128, 128, 3)\n",
            "temp_input:  (1, 128, 128, 3)\n",
            "temp_input:  (1, 128, 128, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-7f6c2bb01655>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd ../../'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-9e1ba14a00f3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    123\u001b[0m       \u001b[0mcheckpoint_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m       \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m       \u001b[0muse_matching_aware_dis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_matching_aware_discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m   )\n",
            "\u001b[0;32m<ipython-input-26-02a5446838a8>\u001b[0m in \u001b[0;36mtrain_networks\u001b[0;34m(encoder, ca, c_pro_gan, dataset, epochs, encoder_optim, ca_optim, fade_in_percentage, batch_sizes, start_depth, num_workers, feedback_factor, log_dir, sample_dir, checkpoint_factor, save_dir, use_matching_aware_dis)\u001b[0m\n\u001b[1;32m    158\u001b[0m                       ),\n\u001b[1;32m    159\u001b[0m                       \u001b[0mscale_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_pro_gan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                       \u001b[0mimg_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_img_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m                   )\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-cf2cd07238db>\u001b[0m in \u001b[0;36mcreate_grid\u001b[0;34m(samples, scale_factor, img_file, real_imgs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;31m# call new captioning method on samples for new loss calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mgenerated_captions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;31m# print(\"Image Caption Loss: \", generated_captions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-c931c69d4137>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m     37\u001b[0m       predictions, hidden, attention_weights = decoder([dec_input,\n\u001b[1;32m     38\u001b[0m                                                         \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                                                         hidden])\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mpredicted_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-34813dc38014>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# x shape after passing through embedding == (batch_size, 1, embedding_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/embeddings.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    194\u001b[0m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m       \u001b[0;31m# Instead of casting the variable as in most layers, cast the output, as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36membedding_lookup_v2\u001b[0;34m(params, ids, max_norm, name)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m   \"\"\"\n\u001b[0;32m--> 394\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36membedding_lookup\u001b[0;34m(params, ids, partition_strategy, name, validate_indices, max_norm)\u001b[0m\n\u001b[1;32m    326\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m       \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m       transform_fn=None)\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36m_embedding_lookup_and_transform\u001b[0;34m(params, ids, partition_strategy, name, max_norm, transform_fn)\u001b[0m\n\u001b[1;32m    142\u001b[0m       \u001b[0;31m# params. Similar to the case np > 1 where parallel_dynamic_stitch is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m       \u001b[0;31m# outside the scioe of all with ops.colocate_with(params[p]).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m       \u001b[0;31m# Flatten the ids. There are two cases where we need to do this.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m   \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m   \u001b[0;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_handle_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m   3928\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3929\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 3930\u001b[0;31m         _ctx, \"Identity\", name, input)\n\u001b[0m\u001b[1;32m   3931\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3932\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}