{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageCaptioningtoImageGeneration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/austin-strom/text2face/blob/main/ImageCaptioningtoImageGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VJirYSWuLxk"
      },
      "source": [
        "# **Starter Notebook to Combine the Image Captioning Network Output to the Image Generator Network to Evaluate Performance of Image Caption**   \n",
        "\n",
        "Final Project - Mikayla Biggs, Kevin Steele, Austin Strom    \n",
        "AML 4/26/2021\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o_LxdNqueIX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxl2jY2bgEMF"
      },
      "source": [
        "### Approach: \n",
        "* pre-trained image captioning network model\n",
        "    * used as forward loss to improve T2F performance\n",
        "    * model essentially used as descriminator\n",
        "* V1 is only forward loss\n",
        "    * V2 forward loss + backward \n",
        "        * training T2F and image captioning at same time in case captioning bottlenecks performance in V1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUG98rcPZYmt"
      },
      "source": [
        "### Retreive Text2Face Repository and Data\n",
        "This is setup to get the v0.1 and v1.0 data from the google drive links. The v1.0 data has not been cleaned so the v0.1 should be used for initial testing of the base project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeLk57l9pVlM"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# You'll generate plots of attention in order to see which parts of an image\n",
        "# our model focuses on during captioning\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz0P6w15ttrd"
      },
      "source": [
        "!git clone https://github.com/austin-strom/T2F.git\n",
        "!gdown --id 1nD6kNAgIVjxpzIScJNLqUyRA1qEkc4Op\n",
        "!gdown --id 1cwcYbl0dhXEzmdbee_K_H6jcndbsxT2o\n",
        "\n",
        "!unzip -u -q face2text_v0.1.zip -d face2text_v0.1\n",
        "!unzip -u -q face2text_v1.0.zip -d face2text_v1.0\n",
        "\n",
        "# # This is moving the v0.1 file to the proper data dir for testing\n",
        "!mkdir T2F/data/LFW/Face2Text\n",
        "!mv face2text_v0.1/ T2F/data/LFW/Face2Text/.\n",
        "\n",
        "!wget http://vis-www.cs.umass.edu/lfw/lfw.tgz\n",
        "!tar -xf lfw.tgz\n",
        "!mv lfw T2F/data/LFW/."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TixEeKykZaDj"
      },
      "source": [
        "# !pip uninstall pro-gan-pth==1.3.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTg8fsLfMZhI"
      },
      "source": [
        "# !git clone 'https://github.com/austin-strom/pro_gan_pytorch.git'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAU53DzypiyU"
      },
      "source": [
        "# # This is moving the v0.1 file to the proper data dir for testing\n",
        "\n",
        "!mkdir face2text_v0.1/data\n",
        "# !mv face2text_v0.1/ Face2Text/.\n",
        "\n",
        "!wget http://vis-www.cs.umass.edu/lfw/lfw.tgz\n",
        "!tar -xf lfw.tgz\n",
        "!mv lfw face2text_v0.1/data/."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdDz2MxxZd0E"
      },
      "source": [
        "%load_ext tensorboard\n",
        "import datetime, os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AKGYTIVdfx_"
      },
      "source": [
        "!rm -rf T2F/implementation/runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RBD7t55uleO"
      },
      "source": [
        "!git clone 'https://github.com/austin-strom/pro_gan_pytorch.git'\n",
        "!cd pro_gan_pytorch && git checkout revert_to_v1_3_3\n",
        "!mv pro_gan_pytorch/pro_gan_pytorch T2F/implementation/pro_gan_pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYfM1irIdiiT"
      },
      "source": [
        "# %cd T2F/implementation/\n",
        "\n",
        "# %tensorboard --logdir=runs\n",
        "\n",
        "# !mkdir training_runs\n",
        "# !mkdir training_runs/generated_samples training_runs/losses training_runs/saved_models\n",
        "# !python3 train_network.py --config=configs/1.conf\n",
        "\n",
        "# %cd ../../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9IpTkslgQWG"
      },
      "source": [
        "import sys\n",
        "import datetime\n",
        "import time\n",
        "import torch as th\n",
        "import numpy as np\n",
        "import argparse\n",
        "import yaml\n",
        "import os\n",
        "import pickle\n",
        "import timeit\n",
        "import tensorflow as tf \n",
        "\n",
        "from torch.backends import cudnn\n",
        "\n",
        "# Append path to sys\n",
        "sys.path.append(\"T2F/implementation\")\n",
        "sys.path.append(\"T2F/implementation/networks\")\n",
        "sys.path.append(\"T2F/implementation/data_processing\")\n",
        "sys.path.append(\"T2F/implementation/pro_gan_pytorch\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4t4vQbokx1V"
      },
      "source": [
        "%cd T2F/implementation/\n",
        "\n",
        "import data_processing.DataLoader as dl\n",
        "\n",
        "# define the device for the training script\n",
        "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# set torch manual seed for consistent output\n",
        "th.manual_seed(3)\n",
        "\n",
        "# Start fast training mode:\n",
        "cudnn.benchmark = True\n",
        "logdir = \"runs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "file_writer = tf.summary.create_file_writer(logdir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGgAHdIJk52U"
      },
      "source": [
        "def  parse_keyword_arguments():\n",
        "  # pass dictionary into main\n",
        "  \"\"\"\n",
        "  command line arguments parser\n",
        "  :return: args => parsed command line arguments\n",
        "  \"\"\"\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument(\"--config\", action=\"store\", type=str, default=\"configs/1.conf\",\n",
        "    help=\"default configuration for the Network\")\n",
        "  parser.add_argument(\"--start_depth\", action=\"store\", type=int, default=0,\n",
        "    help=\"Starting depth for training the network\")\n",
        "  parser.add_argument(\"--encoder_file\", action=\"store\", type=str, default=None,\n",
        "    help=\"pretrained Encoder file (compatible with my code)\")\n",
        "  parser.add_argument(\"--ca_file\", action=\"store\", type=str, default=None,\n",
        "    help=\"pretrained Conditioning Augmentor file (compatible with my code)\")\n",
        "  parser.add_argument(\"--generator_file\", action=\"store\", type=str, default=None,\n",
        "    help=\"pretrained Generator file (compatible with my code)\")\n",
        "  parser.add_argument(\"--discriminator_file\", action=\"store\", type=str, default=None,\n",
        "    help=\"pretrained Discriminator file (compatible with my code)\")\n",
        "\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  return args"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR4dhTsVlEXE"
      },
      "source": [
        "def get_config(conf_file):\n",
        "  \"\"\"\n",
        "  parse and load the provided configuration\n",
        "  :param conf_file: configuration file\n",
        "  :return: conf => parsed configuration\n",
        "  \"\"\"\n",
        "  from easydict import EasyDict as edict\n",
        "\n",
        "  with open(conf_file, \"r\") as file_descriptor:\n",
        "      data = yaml.load(file_descriptor)\n",
        "\n",
        "  # convert the data into an easyDictionary\n",
        "  return edict(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E8oC7CnlIEZ"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "# How the losss works:\n",
        "# Takes batch of what the next word is in the sequence (real) and of the predicted values from the vocab (pred)\n",
        "# real = index of next word\n",
        "# pred = all vocab words ranked by conditional probability that they are the next word\n",
        "# mask = 1 or 0 on whether the value of real is 0 (<pad>)\n",
        "# return mean loss of batch (mask*loss_object(real, pred))\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real.cpu(), 0))\n",
        "  loss_ = loss_object(real.cpu(), pred)\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "\n",
        "def evaluate_caption(images, reals):\n",
        "    # results = []\n",
        "    losses = []\n",
        "    count = 0\n",
        "    for image in images:\n",
        "      hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "      temp_input = tf.expand_dims(load_image(image), 0)\n",
        "      img_tensor_val = image_features_extract_model(temp_input)\n",
        "      img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n",
        "                                                  -1,\n",
        "                                                  img_tensor_val.shape[3]))\n",
        "\n",
        "      # Find Image Features\n",
        "      features = encoder(img_tensor_val)\n",
        "\n",
        "      dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "      # result = []\n",
        "      loss = 0\n",
        "      for i in range(94):\n",
        "          # Find conditional probabilities of each word in vocab\n",
        "          predictions, hidden, attention_weights = decoder([dec_input,\n",
        "                                                          features,\n",
        "                                                          hidden])\n",
        "\n",
        "          loss += loss_function(reals[count][i], predictions)\n",
        "          predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "          # result.append(tokenizer.index_word[predicted_id])\n",
        "          if predicted_id not in tokenizer.index_word.keys():\n",
        "              predicted_id = 0\n",
        "\n",
        "          if tokenizer.index_word[predicted_id] == '<end>':\n",
        "              break\n",
        "\n",
        "          dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      # results.append(result)\n",
        "      losses.append(loss)\n",
        "      count += 1\n",
        "  \n",
        "    numpy_list = np.array([loss.numpy() for loss in losses])\n",
        "    numpy_list.flatten()\n",
        "    loss_val = np.mean(numpy_list)\n",
        "    return loss_val / 100.0\n",
        "\n",
        "\n",
        "def create_grid(samples, scale_factor, img_file, real_imgs=False):\n",
        "  \"\"\"\n",
        "  utility function to create a grid of GAN samples\n",
        "  :param samples: generated samples for storing\n",
        "  :param scale_factor: factor for upscaling the image\n",
        "  :param img_file: name of file to write\n",
        "  :param real_imgs: turn off the scaling of images\n",
        "  :return: None (saves a file)\n",
        "  \"\"\"\n",
        "  from torchvision.utils import save_image\n",
        "  from torch.nn.functional import interpolate\n",
        "\n",
        "  samples = th.clamp((samples / 2) + 0.5, min=0, max=1)\n",
        "  # print(samples)\n",
        "\n",
        "  # upsample the image\n",
        "  if not real_imgs and scale_factor > 1:\n",
        "      samples = interpolate(samples,\n",
        "                            scale_factor=scale_factor)\n",
        "\n",
        "\n",
        "  # save the images:\n",
        "  save_image(samples, img_file, nrow=int(np.sqrt(len(samples))))\n",
        "\n",
        "\n",
        "def generate_caption_loss(samples, scale_factor, captions, real_imgs=False):\n",
        "\n",
        "  from torchvision.utils import save_image\n",
        "  from torch.nn.functional import interpolate\n",
        "\n",
        "  samples = th.clamp((samples / 2) + 0.5, min=0, max=1)\n",
        "  # print(samples)\n",
        "\n",
        "  # upsample the image\n",
        "  if not real_imgs and scale_factor > 1:\n",
        "      samples = interpolate(samples,\n",
        "                            scale_factor=scale_factor)\n",
        "      \n",
        "  # Image Caption Loss\n",
        "  # Tokenize all captions\n",
        "  # real_captions = [[tokenizer.word_index[i] for i in caption.split()] for caption in captions]\n",
        "\n",
        "  # call new captioning method on samples for new loss calculation\n",
        "  # Provides new captions and the individual losses for each image\n",
        "  avg_loss = evaluate_caption(samples.detach(), reals=captions)\n",
        "  return avg_loss\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKjD-zsvlOsd"
      },
      "source": [
        "def create_descriptions_file(file, captions, dataset):\n",
        "  \"\"\"\n",
        "  utility function to create a file for storing the captions\n",
        "  :param file: file for storing the captions\n",
        "  :param captions: encoded_captions or raw captions\n",
        "  :param dataset: the dataset object for transforming captions\n",
        "  :return: None (saves a file)\n",
        "  \"\"\"\n",
        "  from functools import reduce\n",
        "\n",
        "  # transform the captions to text:\n",
        "  if isinstance(captions, th.Tensor):\n",
        "      captions = list(map(lambda x: dataset.get_english_caption(x.cpu()),\n",
        "                          [captions[i] for i in range(captions.shape[0])]))\n",
        "\n",
        "      with open(file, \"w\") as filler:\n",
        "          for caption in captions:\n",
        "              filler.write(reduce(lambda x, y: x + \" \" + y, caption, \"\"))\n",
        "              filler.write(\"\\n\\n\")\n",
        "  else:\n",
        "      with open(file, \"w\") as filler:\n",
        "          for caption in captions:\n",
        "              filler.write(caption)\n",
        "              filler.write(\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJtIJ1aJFwfr"
      },
      "source": [
        "### Add step in loss function to compare intermediate image output result from image captioning network   \n",
        "\n",
        "Acts as adition metric to tune loss and ideally improve generated image quality.   \n",
        "\n",
        "**Where?**    \n",
        "In pr_gan_pytorch project by Akanimax, output from training image generator - currently using wpgan loss. \n",
        "\n",
        "**TODO:**   \n",
        "Can't find where any validation/testing is being done to take intermediate generated images - need to investigate training loop with Austin to see if this can be added to change the loss function and add the image captioning network to the pipeline\n",
        "\n",
        "[ProGAN Loss Functons - Akanimax T2F](https://github.com/akanimax/pro_gan_pytorch/blob/cdd9002ad171ee47c65c3670318473a76eb682e2/pro_gan_pytorch/losses.py#L35)   \n",
        "\n",
        "[Word Importance - Akanimax T2F](https://github.com/austin-strom/T2F/blob/master/implementation/networks/InferSent/encoder/demo.ipynb)   \n",
        "\n",
        "\n",
        "[Evaluate Method - Image Captioning](https://github.com/austin-strom/text2face/blob/main/TFImageCap.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC-A0eZvuZJN"
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, x):\n",
        "    features = x[0]\n",
        "    hidden = x[1]\n",
        "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
        "\n",
        "    # hidden shape == (batch_size, hidden_size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "    # attention_hidden_layer shape == (batch_size, 64, units)\n",
        "    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
        "                                         self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # score shape == (batch_size, 64, 1)\n",
        "    # This gives you an unnormalized score for each image feature.\n",
        "    score = self.V(attention_hidden_layer)\n",
        "\n",
        "    # attention_weights shape == (batch_size, 64, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * features\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96PjfIEqueI7"
      },
      "source": [
        "class RNN_Decoder(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units, vocab_size):\n",
        "    super(RNN_Decoder, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "  def call(self, x):\n",
        "    features = x[1]\n",
        "    hidden = x[2]\n",
        "    x = x[0]\n",
        "    # defining attention as a separate model\n",
        "    context_vector, attention_weights = self.attention([features, hidden])\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # shape == (batch_size, max_length, hidden_size)\n",
        "    x = self.fc1(output)\n",
        "\n",
        "    # x shape == (batch_size * max_length, hidden_size)\n",
        "    x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size * max_length, vocab)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "  def reset_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYgV4EIpuitr"
      },
      "source": [
        "class CNN_Encoder(tf.keras.Model):\n",
        "    # Since you have already extracted the features and dumped it\n",
        "    # This encoder passes those features through a Fully connected layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        # shape after fc == (batch_size, 64, embedding_dim)\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCKal8KViKcN"
      },
      "source": [
        "# Feel free to change these parameters according to your system's configuration\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "vocab_size = 5000 + 1\n",
        "# num_steps = len(img_name_train) // BATCH_SIZE\n",
        "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
        "# These two variables represent that vector shape\n",
        "features_shape = 2048\n",
        "attention_features_shape = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDvh8q0-wli8"
      },
      "source": [
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
        "\n",
        "encoder.built=True\n",
        "decoder.built=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2AL6Vq3vDPD"
      },
      "source": [
        "%cd ../../\n",
        "!pwd\n",
        "!git clone https://github.com/austin-strom/text2face.git\n",
        "!mkdir encoder\n",
        "!mkdir decoder\n",
        "!mv text2face/ImgCapModel/decoder* decoder/.\n",
        "!mv text2face/ImgCapModel/encoder* encoder/.\n",
        "!mv text2face/ImgCapModel/tokenizer.pickle .\n",
        "\n",
        "%cd T2F/implementation/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbG3EVq2jjIk"
      },
      "source": [
        "saving_encoder_path = '/content/encoder/encoder'\n",
        "saving_decoder_path = '/content/decoder/decoder'\n",
        "saving_tokenizer_path = '/content/tokenizer.pickle'\n",
        "load_model = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSIiO4nWi5RO"
      },
      "source": [
        "import pickle as pkl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "# from TFImageCap import RNN_Decoder, CNN_Encoder\n",
        "if load_model:\n",
        "  encoder.load_weights(saving_encoder_path)\n",
        "  decoder.load_weights(saving_decoder_path)\n",
        "\n",
        "  # loading\n",
        "  with open(saving_tokenizer_path, 'rb') as handle:\n",
        "      tokenizer = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaNkXreLjRKQ"
      },
      "source": [
        "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
        "                                                weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOmCgENRFo6P"
      },
      "source": [
        "# compare words generated from image captioning network and original caption to \n",
        "# see if the image generator is preserving key features of the original subject\n",
        "def eval_key_words(im_cap, true_cap):\n",
        "  # 1. identify important words (maybe use model to eval word importance like RF)\n",
        "  # 2. calculate some difference metric like MSE\n",
        "  # 3. return the metric to be used in GAN loss fxn\n",
        "\n",
        "  return\n",
        "\n",
        "# Should be the same as what is used in the image captioning network Kevin worked on\n",
        "# input image is intermediate output from text-to-image GAN generator\n",
        "def load_image(img):\n",
        "  img = np.asarray(img.cpu()).transpose(1,2,0)\n",
        "  img = tf.image.resize(img, (128, 128))\n",
        "  img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "  return img\n",
        "\n",
        "# get the image caption for the intermediate generated output image\n",
        "# NOTE: may need more inputs based on what is needed for the captioning network\n",
        "# NOTE: need to use the evaluate method from TFImageCap notebook to generate caption\n",
        "def evaluate(images):\n",
        "  result = list()\n",
        "\n",
        "  for im in images:\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    temp_input = tf.expand_dims(load_image(im), 0)\n",
        "    print(\"temp_input: \", np.shape(temp_input))\n",
        "    img_tensor_val = image_features_extract_model(temp_input)\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n",
        "                                              -1, img_tensor_val.shape[3]))\n",
        "    features = encoder(img_tensor_val)\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "    current_im_result = []\n",
        "\n",
        "    for i in range(94):\n",
        "      predictions, hidden, attention_weights = decoder([dec_input,\n",
        "                                                        features,\n",
        "                                                        hidden])\n",
        "\n",
        "      predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "\n",
        "      if predicted_id not in tokenizer.index_word.keys():\n",
        "        predicted_id = 0\n",
        "\n",
        "      current_im_result.append(tokenizer.index_word[predicted_id])\n",
        "\n",
        "      if tokenizer.index_word[predicted_id] == '<end>':\n",
        "        result.append(current_im_result)\n",
        "        break\n",
        "\n",
        "      dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    result.append(current_im_result)\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8znkFwAlRiJ"
      },
      "source": [
        "def train_networks(encoder, ca, c_pro_gan, dataset, epochs,\n",
        "                  encoder_optim, ca_optim, fade_in_percentage,\n",
        "                  batch_sizes, start_depth, num_workers, feedback_factor,\n",
        "                  log_dir, sample_dir, checkpoint_factor,\n",
        "                  save_dir, use_matching_aware_dis=True):\n",
        "  # required only for type checking\n",
        "  from networks.TextEncoder import PretrainedEncoder\n",
        "\n",
        "  # input assertions\n",
        "  assert c_pro_gan.depth == len(batch_sizes), \"batch_sizes not compatible with depth\"\n",
        "  assert c_pro_gan.depth == len(epochs), \"epochs_sizes not compatible with depth\"\n",
        "  assert c_pro_gan.depth == len(fade_in_percentage), \"fip_sizes not compatible with depth\"\n",
        "\n",
        "  # put all the Networks in training mode:\n",
        "  ca.train()\n",
        "  c_pro_gan.gen.train()\n",
        "  c_pro_gan.dis.train()\n",
        "\n",
        "  if not isinstance(encoder, PretrainedEncoder):\n",
        "      encoder.train()\n",
        "\n",
        "  print(\"Starting the training process ... \")\n",
        "\n",
        "  # create fixed_input for debugging\n",
        "  temp_data = dl.get_data_loader(dataset, batch_sizes[start_depth], num_workers=3)\n",
        "  fixed_captions, fixed_real_images = iter(temp_data).next()\n",
        "\n",
        "  fixed_real_images = fixed_real_images.to(device)\n",
        "  fixed_captions = fixed_captions.to(device)\n",
        "\n",
        "  fixed_embeddings = encoder(fixed_captions)\n",
        "\n",
        "  fixed_embeddings = (fixed_embeddings).to(device)\n",
        "\n",
        "  fixed_c_not_hats, _, _ = ca(fixed_embeddings)\n",
        "\n",
        "  fixed_noise = th.randn(len(fixed_captions),\n",
        "                          c_pro_gan.latent_size - fixed_c_not_hats.shape[-1]).to(device)\n",
        "\n",
        "  fixed_gan_input = th.cat((fixed_c_not_hats, fixed_noise), dim=-1)\n",
        "\n",
        "  # save the fixed_images once:\n",
        "  fixed_save_dir = os.path.join(sample_dir, \"__Real_Info\")\n",
        "  os.makedirs(fixed_save_dir, exist_ok=True)\n",
        "  create_grid(fixed_real_images, None,  # scale factor is not required here\n",
        "              os.path.join(fixed_save_dir, \"real_samples.png\"), real_imgs=True)\n",
        "  create_descriptions_file(os.path.join(fixed_save_dir, \"real_captions.txt\"),\n",
        "                            fixed_captions,\n",
        "                            dataset)\n",
        "\n",
        "  # create a global time counter\n",
        "  global_time = time.time()\n",
        "\n",
        "  # delete temp data loader:\n",
        "  del temp_data\n",
        "\n",
        "  for current_depth in range(start_depth, c_pro_gan.depth):\n",
        "\n",
        "      print(\"\\n\\nCurrently working on Depth: \", current_depth)\n",
        "      current_res = np.power(2, current_depth + 2)\n",
        "      print(\"Current resolution: %d x %d\" % (current_res, current_res))\n",
        "\n",
        "      data = dl.get_data_loader(dataset, batch_sizes[current_depth], num_workers)\n",
        "\n",
        "      ticker = 1\n",
        "\n",
        "      for epoch in range(1, epochs[current_depth] + 1):\n",
        "          start = timeit.default_timer()  # record time at the start of epoch\n",
        "\n",
        "          print(\"\\nEpoch: %d\" % epoch)\n",
        "          total_batches = len(iter(data))\n",
        "          fader_point = int((fade_in_percentage[current_depth] / 100)\n",
        "                            * epochs[current_depth] * total_batches)\n",
        "\n",
        "          for (i, batch) in enumerate(data, 1):\n",
        "              # calculate the alpha for fading in the layers\n",
        "              alpha = ticker / fader_point if ticker <= fader_point else 1\n",
        "\n",
        "              # extract current batch of data for training\n",
        "              captions, images = batch\n",
        "\n",
        "              if encoder_optim is not None:\n",
        "                  captions = captions.to(device)\n",
        "\n",
        "              images = images.to(device)\n",
        "\n",
        "              # perform text_work:\n",
        "              embeddings = encoder(captions).to(device)\n",
        "              if encoder_optim is None:\n",
        "                  # detach the LSTM from backpropagation\n",
        "                  embeddings = embeddings.detach()\n",
        "              c_not_hats, mus, sigmas = ca(embeddings)\n",
        "\n",
        "              z = th.randn(\n",
        "                  len(captions),\n",
        "                  c_pro_gan.latent_size - c_not_hats.shape[-1]\n",
        "              ).to(device)\n",
        "\n",
        "              gan_input = th.cat((c_not_hats, z), dim=-1)\n",
        "\n",
        "              # optimize the discriminator:\n",
        "              dis_loss = c_pro_gan.optimize_discriminator(gan_input, images,\n",
        "                                                          embeddings.detach(),\n",
        "                                                          current_depth, alpha,\n",
        "                                                          use_matching_aware_dis)\n",
        "\n",
        "              # optimize the generator:\n",
        "              z = th.randn(\n",
        "                  captions.shape[0] if isinstance(captions, th.Tensor) else len(captions),\n",
        "                  c_pro_gan.latent_size - c_not_hats.shape[-1]\n",
        "              ).to(device)\n",
        "\n",
        "              gan_input = th.cat((c_not_hats, z), dim=-1)\n",
        "\n",
        "              if encoder_optim is not None:\n",
        "                  encoder_optim.zero_grad()\n",
        "\n",
        "              ca_optim.zero_grad()\n",
        "\n",
        "              caption_loss = generate_caption_loss( samples=c_pro_gan.gen(fixed_gan_input,\n",
        "                                                                 current_depth,\n",
        "                                                                 alpha),\n",
        "                                          scale_factor=int(np.power(2, c_pro_gan.depth - current_depth - 1)),\n",
        "                                          captions=captions)\n",
        "              gen_loss = c_pro_gan.optimize_generator(gan_input, embeddings,\n",
        "                                                      current_depth, alpha, caption_loss)\n",
        "\n",
        "              # once the optimize_generator is called, it also sends gradients\n",
        "              # to the Conditioning Augmenter and the TextEncoder. Hence the\n",
        "              # zero_grad statements prior to the optimize_generator call\n",
        "              # now perform optimization on those two as well\n",
        "              # obtain the loss (KL divergence from ca_optim)\n",
        "              kl_loss = th.mean(0.5 * th.sum((mus ** 2) + (sigmas ** 2)\n",
        "                                              - th.log((sigmas ** 2)) - 1, dim=1))\n",
        "              kl_loss.backward()\n",
        "              ca_optim.step()\n",
        "              if encoder_optim is not None:\n",
        "                  encoder_optim.step()\n",
        "\n",
        "              # provide a loss feedback\n",
        "              if i % int(total_batches / feedback_factor) == 0 or i == 1:\n",
        "                  elapsed = time.time() - global_time\n",
        "                  elapsed = str(datetime.timedelta(seconds=elapsed))\n",
        "                  print(\"Elapsed [%s]  batch: %d  d_loss: %f  g_loss: %f  kl_los: %f\"\n",
        "                        % (elapsed, i, dis_loss, gen_loss, kl_loss.item()))\n",
        "\n",
        "                  # also write the losses to the log file:\n",
        "                  os.makedirs(log_dir, exist_ok=True)\n",
        "                  log_file = os.path.join(log_dir, \"loss_\" + str(current_depth) + \".log\")\n",
        "                  with open(log_file, \"a\") as log:\n",
        "                      log.write(str(dis_loss) + \"\\t\" + str(gen_loss)\n",
        "                                + \"\\t\" + str(kl_loss.item()) + \"\\n\")\n",
        "\n",
        "                  # create a grid of samples and save it\n",
        "                  gen_img_file = os.path.join(sample_dir, \"gen_\" + str(current_depth) +\n",
        "                                              \"_\" + str(epoch) + \"_\" +\n",
        "                                              str(i) + \".png\")\n",
        "\n",
        "                  create_grid(\n",
        "                      samples=c_pro_gan.gen(\n",
        "                          fixed_gan_input,\n",
        "                          current_depth,\n",
        "                          alpha\n",
        "                      ),\n",
        "                      scale_factor=int(np.power(2, c_pro_gan.depth - current_depth - 1)),\n",
        "                      img_file=gen_img_file,\n",
        "                  )\n",
        "\n",
        "              # increment the ticker:\n",
        "              ticker += 1\n",
        "\n",
        "          stop = timeit.default_timer()\n",
        "          print(\"Time taken for epoch: %.3f secs\" % (stop - start))\n",
        "\n",
        "          if epoch % checkpoint_factor == 0 or epoch == 0:\n",
        "              # save the Model\n",
        "              encoder_save_file = os.path.join(save_dir, \"Encoder_\" +\n",
        "                                                str(current_depth) + \".pth\")\n",
        "              ca_save_file = os.path.join(save_dir, \"Condition_Augmentor_\" +\n",
        "                                          str(current_depth) + \".pth\")\n",
        "              gen_save_file = os.path.join(save_dir, \"GAN_GEN_\" +\n",
        "                                            str(current_depth) + \".pth\")\n",
        "              dis_save_file = os.path.join(save_dir, \"GAN_DIS_\" +\n",
        "                                            str(current_depth) + \".pth\")\n",
        "\n",
        "              os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "              if encoder_optim is not None:\n",
        "                  th.save(encoder.state_dict(), encoder_save_file, pickle)\n",
        "              th.save(ca.state_dict(), ca_save_file, pickle)\n",
        "              th.save(c_pro_gan.gen.state_dict(), gen_save_file, pickle)\n",
        "              th.save(c_pro_gan.dis.state_dict(), dis_save_file, pickle)\n",
        "\n",
        "  print(\"Training completed ...\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jule2aT1gygA"
      },
      "source": [
        "def main(args):\n",
        "  \"\"\"\n",
        "  Main function for the script\n",
        "  :param args: all args from cmdl as a dictionary (key,val) \n",
        "  :return: None\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"Using Device:\", device)\n",
        "  from networks.TextEncoder import Encoder\n",
        "  from networks.ConditionAugmentation import ConditionAugmentor\n",
        "  from pro_gan_pytorch.PRO_GAN import ConditionalProGAN\n",
        "\n",
        "  print(args['config'])\n",
        "  config = get_config(args['config'])\n",
        "  print(\"Current Configuration:\", config)\n",
        "\n",
        "  # create the dataset for training\n",
        "  if config.use_pretrained_encoder:\n",
        "      dataset = dl.RawTextFace2TextDataset(\n",
        "          annots_file=config.annotations_file,\n",
        "          img_dir=config.images_dir,\n",
        "          img_transform=dl.get_transform(config.img_dims)\n",
        "      )\n",
        "      from networks.TextEncoder import PretrainedEncoder\n",
        "      # create a new session object for the pretrained encoder:\n",
        "      text_encoder = PretrainedEncoder(\n",
        "          model_file=config.pretrained_encoder_file,\n",
        "          embedding_file=config.pretrained_embedding_file,\n",
        "          device=device\n",
        "      )\n",
        "      encoder_optim = None\n",
        "  else:\n",
        "      dataset = dl.Face2TextDataset(\n",
        "          pro_pick_file=config.processed_text_file,\n",
        "          img_dir=config.images_dir,\n",
        "          img_transform=dl.get_transform(config.img_dims),\n",
        "          captions_len=config.captions_length\n",
        "      )\n",
        "      text_encoder = Encoder(\n",
        "          embedding_size=config.embedding_size,\n",
        "          vocab_size=dataset.vocab_size,\n",
        "          hidden_size=config.hidden_size,\n",
        "          num_layers=config.num_layers,\n",
        "          device=device\n",
        "      )\n",
        "      encoder_optim = th.optim.Adam(text_encoder.parameters(),\n",
        "                                    lr=config.learning_rate,\n",
        "                                    betas=(config.beta_1, config.beta_2),\n",
        "                                    eps=config.eps)\n",
        "\n",
        "  # create the networks\n",
        "\n",
        "  if args['encoder_file'] is not None:\n",
        "      # Note this should not be used with the pretrained encoder file\n",
        "      print(\"Loading encoder from:\", args['encoder_file'])\n",
        "      text_encoder.load_state_dict(th.load(args['encoder_file']))\n",
        "\n",
        "  condition_augmenter = ConditionAugmentor(\n",
        "      input_size=config.hidden_size,\n",
        "      latent_size=config.ca_out_size,\n",
        "      use_eql=config.use_eql,\n",
        "      device=device\n",
        "  )\n",
        "\n",
        "  if args['ca_file'] is not None:\n",
        "      print(\"Loading conditioning augmenter from:\", args['ca_file'])\n",
        "      condition_augmenter.load_state_dict(th.load(args['ca_file']))\n",
        "\n",
        "  c_pro_gan = ConditionalProGAN(\n",
        "      embedding_size=config.hidden_size,\n",
        "      depth=config.depth,\n",
        "      latent_size=config.latent_size,\n",
        "      compressed_latent_size=config.compressed_latent_size,\n",
        "      learning_rate=config.learning_rate,\n",
        "      beta_1=config.beta_1,\n",
        "      beta_2=config.beta_2,\n",
        "      eps=config.eps,\n",
        "      drift=config.drift,\n",
        "      n_critic=config.n_critic,\n",
        "      use_eql=config.use_eql,\n",
        "      loss=config.loss_function,\n",
        "      use_ema=config.use_ema,\n",
        "      ema_decay=config.ema_decay,\n",
        "      device=device\n",
        "  )\n",
        "\n",
        "  if args['generator_file'] is not None:\n",
        "      print(\"Loading generator from:\", args['generator_file'])\n",
        "      c_pro_gan.gen.load_state_dict(th.load(args['generator_file']))\n",
        "\n",
        "  if args['discriminator_file'] is not None:\n",
        "      print(\"Loading discriminator from:\", args['discriminator_file'])\n",
        "      c_pro_gan.dis.load_state_dict(th.load(args['discriminator_file']))\n",
        "\n",
        "  # create the optimizer for Condition Augmenter separately\n",
        "  ca_optim = th.optim.Adam(condition_augmenter.parameters(),\n",
        "                            lr=config.learning_rate,\n",
        "                            betas=(config.beta_1, config.beta_2),\n",
        "                            eps=config.eps)\n",
        "\n",
        "  print(\"Generator Config:\")\n",
        "  print(c_pro_gan.gen)\n",
        "\n",
        "  print(\"\\nDiscriminator Config:\")\n",
        "  print(c_pro_gan.dis)\n",
        "\n",
        "  # train all the networks\n",
        "  train_networks(\n",
        "      encoder=text_encoder,\n",
        "      ca=condition_augmenter,\n",
        "      c_pro_gan=c_pro_gan,\n",
        "      dataset=dataset,\n",
        "      encoder_optim=encoder_optim,\n",
        "      ca_optim=ca_optim,\n",
        "      epochs=config.epochs,\n",
        "      fade_in_percentage=config.fade_in_percentage,\n",
        "      start_depth=args['start_depth'],\n",
        "      batch_sizes=config.batch_sizes,\n",
        "      num_workers=config.num_workers,\n",
        "      feedback_factor=config.feedback_factor,\n",
        "      log_dir=config.log_dir,\n",
        "      sample_dir=config.sample_dir,\n",
        "      checkpoint_factor=config.checkpoint_factor,\n",
        "      save_dir=config.save_dir,\n",
        "      use_matching_aware_dis=config.use_matching_aware_discriminator\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyJVM9d2vU_K"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHTyLwnYji4_"
      },
      "source": [
        "args = {\n",
        "    'config': 'configs/1.conf',\n",
        "    'start_depth': 0,\n",
        "    'encoder_file': None,\n",
        "    'ca_file': None,\n",
        "    'generator_file': None,\n",
        "    'discriminator_file': None\n",
        "}\n",
        "\n",
        "main(args)\n",
        "\n",
        "%cd ../../"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}